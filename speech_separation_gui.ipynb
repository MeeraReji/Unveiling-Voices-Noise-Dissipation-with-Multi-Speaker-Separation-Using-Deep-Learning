{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKmVEvOxNXz9ig32Ci8PCk",
      "include_colab_link": True
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeeraReji/Unveiling-Voices-Noise-Dissipation-with-Multi-Speaker-Separation-Using-Deep-Learning/blob/main/speech_separation_gui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOivC1wgDkS2"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tensorflow.keras.models import load_model\n",
        "from huggingface_hub import hf_hub_download\n",
        "from speechbrain.inference import SepformerSeparation as Separator\n",
        "import subprocess\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to extract features (MFCCs) with fixed length\n",
        "def extract_features(audio_file, sr=16000, n_mfcc=13, max_length=None):\n",
        "    y, sr = librosa.load(audio_file, sr=sr)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    if max_length:\n",
        "        mfccs = librosa.util.fix_length(mfccs, size=max_length)\n",
        "    return mfccs.T  # Transpose to match model's input shape\n",
        "\n",
        "# Download and load your trained model from Hugging Face\n",
        "model_repo_id = 'maithal/classifier'\n",
        "model_filename = 'trained_model_multi.h5'\n",
        "model_path = hf_hub_download(repo_id=model_repo_id, filename=model_filename)\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Spectral subtraction functions\n",
        "def pad_or_trim(signal, target_length):\n",
        "    if len(signal) > target_length:\n",
        "        return signal[:target_length]\n",
        "    else:\n",
        "        return np.pad(signal, (0, target_length - len(signal)), 'constant')\n",
        "\n",
        "def spectral_subtraction(noisy_signal, noise_signal, sr, alpha=4.0, beta=0.002, smoothing_factor=0.98):\n",
        "    noise_signal = pad_or_trim(noise_signal, len(noisy_signal))\n",
        "    noisy_stft = librosa.stft(noisy_signal)\n",
        "    noise_stft = librosa.stft(noise_signal)\n",
        "    magnitude_noisy = np.abs(noisy_stft)\n",
        "    phase_noisy = np.angle(noisy_stft)\n",
        "    magnitude_noise = np.abs(noise_stft)\n",
        "    magnitude_clean = magnitude_noisy.copy()\n",
        "    noise_estimate = np.mean(magnitude_noise[:, :5], axis=1, keepdims=True)\n",
        "\n",
        "    for t in range(magnitude_noisy.shape[1]):\n",
        "        noise_estimate = smoothing_factor * noise_estimate + (1 - smoothing_factor) * magnitude_noise[:, t:t+1]\n",
        "        magnitude_clean[:, t] = np.maximum(magnitude_noisy[:, t] - alpha * noise_estimate[:, 0], beta * noise_estimate[:, 0])\n",
        "\n",
        "    clean_stft = magnitude_clean * np.exp(1j * phase_noisy)\n",
        "    clean_signal = librosa.istft(clean_stft)\n",
        "    return clean_signal\n",
        "\n",
        "def clear_output_directory(output_dir):\n",
        "    for filename in os.listdir(output_dir):\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            os.unlink(file_path)\n",
        "\n",
        "# Function to separate audio using Sepformer\n",
        "def separate_audio(path, num_speakers):\n",
        "    output_dir = \"/content/output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    clear_output_directory(output_dir)\n",
        "\n",
        "    if num_speakers == 1:\n",
        "        return [path]\n",
        "\n",
        "    elif num_speakers == 2:\n",
        "        model = Separator.from_hparams(source=\"speechbrain/sepformer-wsj02mix\", savedir='pretrained_models/sepformer-wsj02mix')\n",
        "    elif num_speakers == 3:\n",
        "        model = Separator.from_hparams(source=\"speechbrain/sepformer-wsj03mix\", savedir='pretrained_models/sepformer-wsj03mix')\n",
        "    elif num_speakers == 4:\n",
        "        subprocess.run('git clone https://github.com/hahmadraza/speechbrain_48k.git', shell=True)\n",
        "        os.chdir('speechbrain_48k')\n",
        "        subprocess.run('pip install -r requirements.txt', shell=True)\n",
        "        subprocess.run('pip install --editable .', shell=True)\n",
        "        model = Separator.from_hparams(source=\"hahmadraz/sepformer-libri4mix\", savedir='pretrained_models/sepformer-libri4mix-48k/')\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    est_sources = model.separate_file(path=path)\n",
        "    source_paths = []\n",
        "    for i in range(num_speakers):\n",
        "        source_path = os.path.join(output_dir, f\"source{i+1}.wav\")\n",
        "        torchaudio.save(source_path, est_sources[:, :, i].detach().cpu(), 8000)\n",
        "        source_paths.append(source_path)\n",
        "    return source_paths\n",
        "\n",
        "# Main audio processing function\n",
        "def process_audio(audio_file, num_speakers):\n",
        "    # Extract features\n",
        "    max_length = 700\n",
        "    sample_features = extract_features(audio_file, max_length=max_length)\n",
        "    sample_features = sample_features[np.newaxis, ..., np.newaxis]\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(sample_features)\n",
        "    predicted_label = np.argmax(predictions)\n",
        "    class_labels = ['Meow', 'Bark', 'Cough', 'Computer_keyboard']\n",
        "    predicted_class = class_labels[predicted_label]\n",
        "\n",
        "    # Clone and set up Waveformer if not already done\n",
        "    if not os.path.exists('Waveformer'):\n",
        "        subprocess.run('git clone https://github.com/vb000/Waveformer', shell=True)\n",
        "    os.chdir('Waveformer')\n",
        "    subprocess.run('pip install -r requirements.txt', shell=True)\n",
        "    subprocess.run('pip install wget', shell=True)\n",
        "    subprocess.run('pip install speechbrain', shell=True)\n",
        "    subprocess.run('pip install torchmetrics', shell=True)\n",
        "\n",
        "    # Convert the input audio to mono\n",
        "    y, sr = librosa.load(audio_file, sr=None, mono=True)\n",
        "    mono_audio_file = '/content/mono_audio.wav'\n",
        "    sf.write(mono_audio_file, y, sr)\n",
        "\n",
        "    # Run Waveformer model with the predicted class\n",
        "    waveformer_input = f'python Waveformer.py \"{mono_audio_file}\" /content/output_audio.wav --targets \"{predicted_class}\"'\n",
        "    subprocess.run(waveformer_input, shell=True)\n",
        "\n",
        "    # Load the noise output and apply spectral subtraction\n",
        "    noise_signal, sr = librosa.load('/content/output_audio.wav', sr=None)\n",
        "    noisy_signal, _ = librosa.load(audio_file, sr=None)\n",
        "    clean_signal = spectral_subtraction(noisy_signal, noise_signal, sr)\n",
        "\n",
        "    # Save the clean audio\n",
        "    clean_audio_path = '/content/clean_signal.wav'\n",
        "    sf.write(clean_audio_path, clean_signal, sr)\n",
        "\n",
        "    # Separate audio sources\n",
        "    separated_sources = separate_audio(clean_audio_path, num_speakers)\n",
        "    return separated_sources\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        gr.Audio(type=\"filepath\", label=\"Upload Audio File\"),\n",
        "        gr.Number(value=1, label=\"Number of Speakers (1-4)\", precision=0, step=1)\n",
        "    ],\n",
        "    outputs=gr.Files(),  # Use gr.Files() to handle multiple output files\n",
        "    title=\"Audio Separation Tool\",\n",
        "    description=\"Upload an audio file to separate different sound sources.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    }
  ]
}
