import gradio as gr
import os
import torch
import torchaudio
import numpy as np
import librosa
from tensorflow.keras.models import load_model
from huggingface_hub import hf_hub_download
from speechbrain.inference import SepformerSeparation as Separator
import subprocess
import soundfile as sf
import matplotlib.pyplot as plt

# Function to extract features (MFCCs) with fixed length
def extract_features(audio_file, sr=16000, n_mfcc=13, max_length=None):
    y, sr = librosa.load(audio_file, sr=sr)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    if max_length:
        mfccs = librosa.util.fix_length(mfccs, size=max_length)
    return mfccs.T  # Transpose to match model's input shape

# Download and load your trained model from Hugging Face
model_repo_id = 'maithal/classifier'
model_filename = 'trained_model_multi.h5'
model_path = hf_hub_download(repo_id=model_repo_id, filename=model_filename)
model = load_model(model_path)

# Spectral subtraction functions
def pad_or_trim(signal, target_length):
    if len(signal) > target_length:
        return signal[:target_length]
    else:
        return np.pad(signal, (0, target_length - len(signal)), 'constant')

def spectral_subtraction(noisy_signal, noise_signal, sr, alpha=4.0, beta=0.002, smoothing_factor=0.98):
    noise_signal = pad_or_trim(noise_signal, len(noisy_signal))
    noisy_stft = librosa.stft(noisy_signal)
    noise_stft = librosa.stft(noise_signal)
    magnitude_noisy = np.abs(noisy_stft)
    phase_noisy = np.angle(noisy_stft)
    magnitude_noise = np.abs(noise_stft)
    magnitude_clean = magnitude_noisy.copy()
    noise_estimate = np.mean(magnitude_noise[:, :5], axis=1, keepdims=True)

    for t in range(magnitude_noisy.shape[1]):
        noise_estimate = smoothing_factor * noise_estimate + (1 - smoothing_factor) * magnitude_noise[:, t:t+1]
        magnitude_clean[:, t] = np.maximum(magnitude_noisy[:, t] - alpha * noise_estimate[:, 0], beta * noise_estimate[:, 0])

    clean_stft = magnitude_clean * np.exp(1j * phase_noisy)
    clean_signal = librosa.istft(clean_stft)
    return clean_signal

def clear_output_directory(output_dir):
    for filename in os.listdir(output_dir):
        file_path = os.path.join(output_dir, filename)
        if os.path.isfile(file_path):
            os.unlink(file_path)

# Function to separate audio using Sepformer
def separate_audio(path, num_speakers):
    output_dir = "/content/output"
    os.makedirs(output_dir, exist_ok=True)
    clear_output_directory(output_dir)

    if num_speakers == 1:
        return [path]

    elif num_speakers == 2:
        model = Separator.from_hparams(source="speechbrain/sepformer-wsj02mix", savedir='pretrained_models/sepformer-wsj02mix')
    elif num_speakers == 3:
        model = Separator.from_hparams(source="speechbrain/sepformer-wsj03mix", savedir='pretrained_models/sepformer-wsj03mix')
    elif num_speakers == 4:
        subprocess.run('git clone https://github.com/hahmadraza/speechbrain_48k.git', shell=True)
        os.chdir('speechbrain_48k')
        subprocess.run('pip install -r requirements.txt', shell=True)
        subprocess.run('pip install --editable .', shell=True)
        model = Separator.from_hparams(source="hahmadraz/sepformer-libri4mix", savedir='pretrained_models/sepformer-libri4mix-48k/')
    else:
        return None

    est_sources = model.separate_file(path=path)
    source_paths = []
    for i in range(num_speakers):
        source_path = os.path.join(output_dir, f"source{i+1}.wav")
        torchaudio.save(source_path, est_sources[:, :, i].detach().cpu(), 8000)
        source_paths.append(source_path)
    return source_paths

# Main audio processing function
def process_audio(audio_file, num_speakers):
    # Extract features
    max_length = 700
    sample_features = extract_features(audio_file, max_length=max_length)
    sample_features = sample_features[np.newaxis, ..., np.newaxis]

    # Make predictions
    predictions = model.predict(sample_features)
    predicted_label = np.argmax(predictions)
    class_labels = ['Meow', 'Bark', 'Cough', 'Computer_keyboard']
    predicted_class = class_labels[predicted_label]

    # Clone and set up Waveformer if not already done
    if not os.path.exists('Waveformer'):
        subprocess.run('git clone https://github.com/vb000/Waveformer', shell=True)
    os.chdir('Waveformer')
    subprocess.run('pip install -r requirements.txt', shell=True)
    subprocess.run('pip install wget', shell=True)
    subprocess.run('pip install speechbrain', shell=True)
    subprocess.run('pip install torchmetrics', shell=True)

    # Convert the input audio to mono
    y, sr = librosa.load(audio_file, sr=None, mono=True)
    mono_audio_file = '/content/mono_audio.wav'
    sf.write(mono_audio_file, y, sr)

    # Run Waveformer model with the predicted class
    waveformer_input = f'python Waveformer.py "{mono_audio_file}" /content/output_audio.wav --targets "{predicted_class}"'
    subprocess.run(waveformer_input, shell=True)

    # Load the noise output and apply spectral subtraction
    noise_signal, sr = librosa.load('/content/output_audio.wav', sr=None)
    noisy_signal, _ = librosa.load(audio_file, sr=None)
    clean_signal = spectral_subtraction(noisy_signal, noise_signal, sr)

    # Save the clean audio
    clean_audio_path = '/content/clean_signal.wav'
    sf.write(clean_audio_path, clean_signal, sr)

    # Separate audio sources
    separated_sources = separate_audio(clean_audio_path, num_speakers)
    return separated_sources

# Gradio Interface
iface = gr.Interface(
    fn=process_audio,
    inputs=[
        gr.Audio(type="filepath", label="Upload Audio File"),
        gr.Number(value=1, label="Number of Speakers (1-4)", precision=0, step=1)
    ],
    outputs=gr.Files(),  # Use gr.Files() to handle multiple output files
    title="Audio Separation Tool",
    description="Upload an audio file to separate different sound sources."
)

iface.launch(debug=True)
